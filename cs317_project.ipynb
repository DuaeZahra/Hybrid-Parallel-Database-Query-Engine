{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DuaeZahra/Hybrid-Parallel-Database-Query-Engine/blob/main/cs317_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0VZ72mefde6"
      },
      "outputs": [],
      "source": [
        "##using pthreads and cuda"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opYyDlwVfi9I",
        "outputId": "3842512b-caf8-4fe1-e845-59d88d95f100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Sun May  4 09:47:53 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install any additional packages if needed\n",
        "!pip install numpy pandas matplotlib\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "# Create a directory for our C++ and CUDA source files\n",
        "!mkdir -p db_engine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMzwxey_HvvX",
        "outputId": "29372c51-30ee-4916-a3b7-be3179386871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the Pthread Implementation (CPU)"
      ],
      "metadata": {
        "id": "spQTjNLXH2R_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile db_engine/pthread_query.cpp\n",
        "#include <pthread.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "#include <unistd.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define MAX_THREADS 16\n",
        "#define MAX_DATA_SIZE 10000000\n",
        "\n",
        "// Shared data structure\n",
        "typedef struct {\n",
        "    int* data;\n",
        "    int start;\n",
        "    int end;\n",
        "    int filter_value;\n",
        "    int* results;\n",
        "    int* result_count;\n",
        "    pthread_mutex_t* mutex;\n",
        "} thread_data_t;\n",
        "\n",
        "// Simple filter operation (similar to WHERE clause in SQL)\n",
        "void* filter_operation(void* arg) {\n",
        "    thread_data_t* thread_data = (thread_data_t*)arg;\n",
        "    int local_count = 0;\n",
        "    int* local_results = (int*)malloc(sizeof(int) * (thread_data->end - thread_data->start));\n",
        "\n",
        "    // Process the assigned chunk of data\n",
        "    for (int i = thread_data->start; i < thread_data->end; i++) {\n",
        "        if (thread_data->data[i] > thread_data->filter_value) {\n",
        "            local_results[local_count++] = thread_data->data[i];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Critical section - update shared results\n",
        "    pthread_mutex_lock(thread_data->mutex);\n",
        "    memcpy(thread_data->results + *thread_data->result_count,\n",
        "           local_results,\n",
        "           local_count * sizeof(int));\n",
        "    *thread_data->result_count += local_count;\n",
        "    pthread_mutex_unlock(thread_data->mutex);\n",
        "\n",
        "    free(local_results);\n",
        "    return NULL;\n",
        "}\n",
        "\n",
        "// Main function that executes parallel query using pthreads\n",
        "extern \"C\" int execute_pthread_query(int* data, int data_size, int filter_value,\n",
        "                                    int* results, int num_threads) {\n",
        "    pthread_t threads[MAX_THREADS];\n",
        "    thread_data_t thread_data[MAX_THREADS];\n",
        "    pthread_mutex_t mutex;\n",
        "    int result_count = 0;\n",
        "\n",
        "    // Initialize mutex\n",
        "    pthread_mutex_init(&mutex, NULL);\n",
        "\n",
        "    // Calculate chunk size for each thread\n",
        "    int chunk_size = data_size / num_threads;\n",
        "\n",
        "    // Create threads\n",
        "    for (int i = 0; i < num_threads; i++) {\n",
        "        thread_data[i].data = data;\n",
        "        thread_data[i].start = i * chunk_size;\n",
        "        thread_data[i].end = (i == num_threads - 1) ? data_size : (i + 1) * chunk_size;\n",
        "        thread_data[i].filter_value = filter_value;\n",
        "        thread_data[i].results = results;\n",
        "        thread_data[i].result_count = &result_count;\n",
        "        thread_data[i].mutex = &mutex;\n",
        "\n",
        "        pthread_create(&threads[i], NULL, filter_operation, &thread_data[i]);\n",
        "    }\n",
        "\n",
        "    // Join threads\n",
        "    for (int i = 0; i < num_threads; i++) {\n",
        "        pthread_join(threads[i], NULL);\n",
        "    }\n",
        "\n",
        "    // Clean up\n",
        "    pthread_mutex_destroy(&mutex);\n",
        "\n",
        "    return result_count;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2hHIuUUH6yp",
        "outputId": "8dfb75e2-db5c-4e20-c307-c0f40aec7ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing db_engine/pthread_query.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the CUDA Implementation (GPU)"
      ],
      "metadata": {
        "id": "iswSW4efIFQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile db_engine/cuda_query.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// CUDA kernel for filtering data (similar to WHERE clause)\n",
        "__global__ void filter_kernel(int* data, int data_size, int filter_value,\n",
        "                              int* results, int* result_count) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (idx < data_size) {\n",
        "        if (data[idx] > filter_value) {\n",
        "            // Atomic operation to get the next available position in results array\n",
        "            int position = atomicAdd(result_count, 1);\n",
        "            results[position] = data[idx];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// Main function that executes query on GPU\n",
        "extern \"C\" int execute_cuda_query(int* h_data, int data_size, int filter_value,\n",
        "                                 int* h_results) {\n",
        "    int *d_data, *d_results, *d_result_count;\n",
        "    int h_result_count = 0;\n",
        "\n",
        "    // Allocate memory on GPU\n",
        "    cudaMalloc((void**)&d_data, sizeof(int) * data_size);\n",
        "    cudaMalloc((void**)&d_results, sizeof(int) * data_size); // Worst case: all elements pass filter\n",
        "    cudaMalloc((void**)&d_result_count, sizeof(int));\n",
        "\n",
        "    // Copy data to GPU\n",
        "    cudaMemcpy(d_data, h_data, sizeof(int) * data_size, cudaMemcpyHostToDevice);\n",
        "    cudaMemset(d_result_count, 0, sizeof(int));\n",
        "\n",
        "    // Configure kernel execution\n",
        "    int block_size = 256;\n",
        "    int grid_size = (data_size + block_size - 1) / block_size;\n",
        "\n",
        "    // Execute kernel\n",
        "    filter_kernel<<<grid_size, block_size>>>(d_data, data_size, filter_value,\n",
        "                                           d_results, d_result_count);\n",
        "\n",
        "    // Copy result count back to host\n",
        "    cudaMemcpy(&h_result_count, d_result_count, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Copy results back to host if any\n",
        "    if (h_result_count > 0) {\n",
        "        cudaMemcpy(h_results, d_results, sizeof(int) * h_result_count, cudaMemcpyDeviceToHost);\n",
        "    }\n",
        "\n",
        "    // Free GPU memory\n",
        "    cudaFree(d_data);\n",
        "    cudaFree(d_results);\n",
        "    cudaFree(d_result_count);\n",
        "\n",
        "    return h_result_count;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fW8amxCIJco",
        "outputId": "895c4f7f-43e3-4ce7-d127-46828b15694f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting db_engine/cuda_query.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the Hybrid Implementation (CPU + GPU)"
      ],
      "metadata": {
        "id": "0elB69fcIPol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile db_engine/hybrid_query.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <pthread.h>\n",
        "\n",
        "#define MAX_THREADS 16\n",
        "\n",
        "// Thread data structure for CPU work\n",
        "typedef struct {\n",
        "    int* data;\n",
        "    int start;\n",
        "    int end;\n",
        "    int filter_value;\n",
        "    int* results;\n",
        "    int* result_count;\n",
        "    pthread_mutex_t* mutex;\n",
        "} thread_data_t;\n",
        "\n",
        "// CPU worker function for Pthreads\n",
        "void* filter_operation_cpu(void* arg) {\n",
        "    thread_data_t* thread_data = (thread_data_t*)arg;\n",
        "    int local_count = 0;\n",
        "    int* local_results = (int*)malloc(sizeof(int) * (thread_data->end - thread_data->start));\n",
        "\n",
        "    // Process the assigned chunk of data\n",
        "    for (int i = thread_data->start; i < thread_data->end; i++) {\n",
        "        if (thread_data->data[i] > thread_data->filter_value) {\n",
        "            local_results[local_count++] = thread_data->data[i];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Critical section - update shared results\n",
        "    pthread_mutex_lock(thread_data->mutex);\n",
        "    memcpy(thread_data->results + *thread_data->result_count,\n",
        "           local_results,\n",
        "           local_count * sizeof(int));\n",
        "    *thread_data->result_count += local_count;\n",
        "    pthread_mutex_unlock(thread_data->mutex);\n",
        "\n",
        "    free(local_results);\n",
        "    return NULL;\n",
        "}\n",
        "\n",
        "// CUDA kernel for filtering data\n",
        "__global__ void filter_kernel_gpu(int* data, int data_size, int filter_value,\n",
        "                              int* results, int* result_count) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (idx < data_size) {\n",
        "        if (data[idx] > filter_value) {\n",
        "            // Atomic operation to get the next available position in results array\n",
        "            int position = atomicAdd(result_count, 1);\n",
        "            results[position] = data[idx];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// Main function that executes hybrid query\n",
        "extern \"C\" int execute_hybrid_query(int* data, int data_size, int filter_value,\n",
        "                                   int* results, int cpu_threads, float gpu_ratio) {\n",
        "\n",
        "    // Determine amount of work for GPU and CPU\n",
        "    int gpu_size = (int)(data_size * gpu_ratio);\n",
        "    int cpu_size = data_size - gpu_size;\n",
        "\n",
        "    int cpu_result_count = 0;\n",
        "    int gpu_result_count = 0;\n",
        "    int* gpu_results = (int*)malloc(sizeof(int) * gpu_size); // For GPU results\n",
        "\n",
        "    // ===== GPU PORTION =====\n",
        "    int *d_data, *d_results, *d_result_count;\n",
        "\n",
        "    // Allocate memory on GPU\n",
        "    cudaMalloc((void**)&d_data, sizeof(int) * gpu_size);\n",
        "    cudaMalloc((void**)&d_results, sizeof(int) * gpu_size); // Worst case: all elements pass filter\n",
        "    cudaMalloc((void**)&d_result_count, sizeof(int));\n",
        "\n",
        "    // Copy data to GPU\n",
        "    cudaMemcpy(d_data, data, sizeof(int) * gpu_size, cudaMemcpyHostToDevice);\n",
        "    cudaMemset(d_result_count, 0, sizeof(int));\n",
        "\n",
        "    // Configure kernel execution\n",
        "    int block_size = 256;\n",
        "    int grid_size = (gpu_size + block_size - 1) / block_size;\n",
        "\n",
        "    // ===== CPU PORTION =====\n",
        "    pthread_t threads[MAX_THREADS];\n",
        "    thread_data_t thread_data[MAX_THREADS];\n",
        "    pthread_mutex_t mutex;\n",
        "\n",
        "    // Initialize mutex\n",
        "    pthread_mutex_init(&mutex, NULL);\n",
        "\n",
        "    // Calculate chunk size for each CPU thread\n",
        "    int chunk_size = cpu_size / cpu_threads;\n",
        "\n",
        "    // Create CPU threads\n",
        "    for (int i = 0; i < cpu_threads; i++) {\n",
        "        thread_data[i].data = data + gpu_size; // Start after GPU portion\n",
        "        thread_data[i].start = i * chunk_size;\n",
        "        thread_data[i].end = (i == cpu_threads - 1) ? cpu_size : (i + 1) * chunk_size;\n",
        "        thread_data[i].filter_value = filter_value;\n",
        "        thread_data[i].results = results; // CPU results go directly to final results array\n",
        "        thread_data[i].result_count = &cpu_result_count;\n",
        "        thread_data[i].mutex = &mutex;\n",
        "\n",
        "        pthread_create(&threads[i], NULL, filter_operation_cpu, &thread_data[i]);\n",
        "    }\n",
        "\n",
        "    // Execute GPU kernel in parallel with CPU threads\n",
        "    filter_kernel_gpu<<<grid_size, block_size>>>(d_data, gpu_size, filter_value,\n",
        "                                             d_results, d_result_count);\n",
        "\n",
        "    // Join CPU threads\n",
        "    for (int i = 0; i < cpu_threads; i++) {\n",
        "        pthread_join(threads[i], NULL);\n",
        "    }\n",
        "\n",
        "    // Copy GPU result count back to host\n",
        "    cudaMemcpy(&gpu_result_count, d_result_count, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Copy GPU results back to host if any\n",
        "    if (gpu_result_count > 0) {\n",
        "        cudaMemcpy(gpu_results, d_results, sizeof(int) * gpu_result_count, cudaMemcpyDeviceToHost);\n",
        "\n",
        "        // Copy GPU results to the final results array (after CPU results)\n",
        "        memcpy(results + cpu_result_count, gpu_results, sizeof(int) * gpu_result_count);\n",
        "    }\n",
        "\n",
        "    // Free GPU memory\n",
        "    cudaFree(d_data);\n",
        "    cudaFree(d_results);\n",
        "    cudaFree(d_result_count);\n",
        "    free(gpu_results);\n",
        "\n",
        "    // Destroy mutex\n",
        "    pthread_mutex_destroy(&mutex);\n",
        "\n",
        "    // Return total number of results\n",
        "    return cpu_result_count + gpu_result_count;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kLZfrwuIQ2E",
        "outputId": "5235f972-f78c-465b-aa52-1673f14a5d3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing db_engine/hybrid_query.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compile the Code"
      ],
      "metadata": {
        "id": "p1bVAtiIIXM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile our code with nvcc - added -fPIC flag\n",
        "!cd db_engine && nvcc -c cuda_query.cu -o cuda_query.o -Xcompiler -fPIC\n",
        "!cd db_engine && nvcc -c hybrid_query.cu -o hybrid_query.o -Xcompiler -fPIC\n",
        "!cd db_engine && g++ -c pthread_query.cpp -o pthread_query.o -fPIC -lpthread\n",
        "\n",
        "# Create shared libraries\n",
        "!cd db_engine && g++ -shared -o libpthread_query.so pthread_query.o -lpthread\n",
        "!cd db_engine && nvcc -shared -o libcuda_query.so cuda_query.o\n",
        "!cd db_engine && nvcc -shared -o libhybrid_query.so hybrid_query.o -lpthread"
      ],
      "metadata": {
        "id": "cgEs_oYgIYQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Libraries in Python"
      ],
      "metadata": {
        "id": "5tkWDynYI1aO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the libraries in Python\n",
        "import ctypes\n",
        "import numpy as np\n",
        "\n",
        "# Load the shared libraries\n",
        "pthread_lib = ctypes.CDLL('./db_engine/libpthread_query.so')\n",
        "cuda_lib = ctypes.CDLL('./db_engine/libcuda_query.so')\n",
        "hybrid_lib = ctypes.CDLL('./db_engine/libhybrid_query.so')\n",
        "\n",
        "# Define argument and return types for the C functions\n",
        "pthread_lib.execute_pthread_query.argtypes = [\n",
        "    np.ctypeslib.ndpointer(dtype=np.int32, ndim=1, flags='C_CONTIGUOUS'),\n",
        "    ctypes.c_int,\n",
        "    ctypes.c_int,\n",
        "    np.ctypeslib.ndpointer(dtype=np.int32, ndim=1, flags='C_CONTIGUOUS'),\n",
        "    ctypes.c_int\n",
        "]\n",
        "pthread_lib.execute_pthread_query.restype = ctypes.c_int\n",
        "\n",
        "cuda_lib.execute_cuda_query.argtypes = [\n",
        "    np.ctypeslib.ndpointer(dtype=np.int32, ndim=1, flags='C_CONTIGUOUS'),\n",
        "    ctypes.c_int,\n",
        "    ctypes.c_int,\n",
        "    np.ctypeslib.ndpointer(dtype=np.int32, ndim=1, flags='C_CONTIGUOUS')\n",
        "]\n",
        "cuda_lib.execute_cuda_query.restype = ctypes.c_int\n",
        "\n",
        "hybrid_lib.execute_hybrid_query.argtypes = [\n",
        "    np.ctypeslib.ndpointer(dtype=np.int32, ndim=1, flags='C_CONTIGUOUS'),\n",
        "    ctypes.c_int,\n",
        "    ctypes.c_int,\n",
        "    np.ctypeslib.ndpointer(dtype=np.int32, ndim=1, flags='C_CONTIGUOUS'),\n",
        "    ctypes.c_int,\n",
        "    ctypes.c_float\n",
        "]\n",
        "hybrid_lib.execute_hybrid_query.restype = ctypes.c_int"
      ],
      "metadata": {
        "id": "nMFSGlJ_I2u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Testing Functions"
      ],
      "metadata": {
        "id": "uHGBzK5NJgL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate some test data (simulating a database table)\n",
        "def generate_test_data(size):\n",
        "    return np.random.randint(0, 1000, size=size, dtype=np.int32)\n",
        "\n",
        "# Test function for Pthread implementation\n",
        "def test_pthread(data, filter_value, num_threads):\n",
        "    results = np.zeros(len(data), dtype=np.int32)\n",
        "    start_time = time.time()\n",
        "    result_count = pthread_lib.execute_pthread_query(data, len(data), filter_value, results, num_threads)\n",
        "    end_time = time.time()\n",
        "    return results[:result_count], end_time - start_time\n",
        "\n",
        "# Test function for CUDA implementation\n",
        "def test_cuda(data, filter_value):\n",
        "    results = np.zeros(len(data), dtype=np.int32)\n",
        "    start_time = time.time()\n",
        "    result_count = cuda_lib.execute_cuda_query(data, len(data), filter_value, results)\n",
        "    end_time = time.time()\n",
        "    return results[:result_count], end_time - start_time\n",
        "\n",
        "# Test function for Hybrid implementation\n",
        "def test_hybrid(data, filter_value, cpu_threads, gpu_ratio):\n",
        "    results = np.zeros(len(data), dtype=np.int32)\n",
        "    start_time = time.time()\n",
        "    result_count = hybrid_lib.execute_hybrid_query(data, len(data), filter_value, results, cpu_threads, gpu_ratio)\n",
        "    end_time = time.time()\n",
        "    return results[:result_count], end_time - start_time"
      ],
      "metadata": {
        "id": "c7WmlzyJJuGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance Testing"
      ],
      "metadata": {
        "id": "WTz1YqqRJw7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run performance tests\n",
        "data_sizes = [100000, 500000, 1000000, 5000000]\n",
        "filter_value = 500  # Select elements > 500\n",
        "num_threads = 4     # Number of CPU threads to use\n",
        "gpu_ratios = [0.0, 0.25, 0.5, 0.75, 1.0]  # Different GPU/CPU workload distributions\n",
        "\n",
        "# Results storage\n",
        "pthread_times = []\n",
        "cuda_times = []\n",
        "hybrid_times = []\n",
        "\n",
        "# Run tests for different data sizes\n",
        "for size in data_sizes:\n",
        "    print(f\"\\nTesting with data size: {size}\")\n",
        "\n",
        "    # Generate test data\n",
        "    data = generate_test_data(size)\n",
        "\n",
        "    # Test Pthread implementation\n",
        "    pthread_results, pthread_time = test_pthread(data, filter_value, num_threads)\n",
        "    pthread_times.append(pthread_time)\n",
        "    print(f\"Pthread execution time: {pthread_time:.4f} seconds, Found {len(pthread_results)} matches\")\n",
        "\n",
        "    # Test CUDA implementation\n",
        "    cuda_results, cuda_time = test_cuda(data, filter_value)\n",
        "    cuda_times.append(cuda_time)\n",
        "    print(f\"CUDA execution time: {cuda_time:.4f} seconds, Found {len(cuda_results)} matches\")\n",
        "\n",
        "    # Test Hybrid implementation with different GPU/CPU ratios\n",
        "    best_hybrid_time = float('inf')\n",
        "    best_ratio = 0.0\n",
        "\n",
        "    for ratio in gpu_ratios:\n",
        "        hybrid_results, hybrid_time = test_hybrid(data, filter_value, num_threads, ratio)\n",
        "        print(f\"  Hybrid (GPU ratio {ratio:.2f}) execution time: {hybrid_time:.4f} seconds, Found {len(hybrid_results)} matches\")\n",
        "\n",
        "        if hybrid_time < best_hybrid_time:\n",
        "            best_hybrid_time = hybrid_time\n",
        "            best_ratio = ratio\n",
        "\n",
        "    hybrid_times.append(best_hybrid_time)\n",
        "    print(f\"Best hybrid performance with GPU ratio {best_ratio:.2f}: {best_hybrid_time:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnzfypNJJ0w3",
        "outputId": "2cb1f6a8-187f-4b5e-c1b3-012e7781ff7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing with data size: 100000\n",
            "Pthread execution time: 0.0021 seconds, Found 50046 matches\n",
            "CUDA execution time: 0.3921 seconds, Found 0 matches\n",
            "  Hybrid (GPU ratio 0.00) execution time: 0.0054 seconds, Found 50046 matches\n",
            "  Hybrid (GPU ratio 0.25) execution time: 0.0009 seconds, Found 37494 matches\n",
            "  Hybrid (GPU ratio 0.50) execution time: 0.0008 seconds, Found 25036 matches\n",
            "  Hybrid (GPU ratio 0.75) execution time: 0.0006 seconds, Found 12444 matches\n",
            "  Hybrid (GPU ratio 1.00) execution time: 0.0005 seconds, Found 0 matches\n",
            "Best hybrid performance with GPU ratio 1.00: 0.0005 seconds\n",
            "\n",
            "Testing with data size: 500000\n",
            "Pthread execution time: 0.0031 seconds, Found 249122 matches\n",
            "CUDA execution time: 0.0010 seconds, Found 0 matches\n",
            "  Hybrid (GPU ratio 0.00) execution time: 0.0035 seconds, Found 249122 matches\n",
            "  Hybrid (GPU ratio 0.25) execution time: 0.0029 seconds, Found 186692 matches\n",
            "  Hybrid (GPU ratio 0.50) execution time: 0.0020 seconds, Found 124514 matches\n",
            "  Hybrid (GPU ratio 0.75) execution time: 0.0015 seconds, Found 62101 matches\n",
            "  Hybrid (GPU ratio 1.00) execution time: 0.0010 seconds, Found 0 matches\n",
            "Best hybrid performance with GPU ratio 1.00: 0.0010 seconds\n",
            "\n",
            "Testing with data size: 1000000\n",
            "Pthread execution time: 0.0070 seconds, Found 499236 matches\n",
            "CUDA execution time: 0.0017 seconds, Found 0 matches\n",
            "  Hybrid (GPU ratio 0.00) execution time: 0.0067 seconds, Found 499236 matches\n",
            "  Hybrid (GPU ratio 0.25) execution time: 0.0049 seconds, Found 374674 matches\n",
            "  Hybrid (GPU ratio 0.50) execution time: 0.0037 seconds, Found 249717 matches\n",
            "  Hybrid (GPU ratio 0.75) execution time: 0.0029 seconds, Found 124848 matches\n",
            "  Hybrid (GPU ratio 1.00) execution time: 0.0017 seconds, Found 0 matches\n",
            "Best hybrid performance with GPU ratio 1.00: 0.0017 seconds\n",
            "\n",
            "Testing with data size: 5000000\n",
            "Pthread execution time: 0.0338 seconds, Found 2492403 matches\n",
            "CUDA execution time: 0.0067 seconds, Found 0 matches\n",
            "  Hybrid (GPU ratio 0.00) execution time: 0.0332 seconds, Found 2492403 matches\n",
            "  Hybrid (GPU ratio 0.25) execution time: 0.0270 seconds, Found 1868719 matches\n",
            "  Hybrid (GPU ratio 0.50) execution time: 0.0221 seconds, Found 1245389 matches\n",
            "  Hybrid (GPU ratio 0.75) execution time: 0.0124 seconds, Found 622876 matches\n",
            "  Hybrid (GPU ratio 1.00) execution time: 0.0065 seconds, Found 0 matches\n",
            "Best hybrid performance with GPU ratio 1.00: 0.0065 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance Visualization"
      ],
      "metadata": {
        "id": "f-dLC9FqJ4j6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot performance comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(data_sizes, pthread_times, 'b-o', label='Pthread (CPU only)')\n",
        "plt.plot(data_sizes, cuda_times, 'r-s', label='CUDA (GPU only)')\n",
        "plt.plot(data_sizes, hybrid_times, 'g-^', label='Hybrid (Best ratio)')\n",
        "plt.xlabel('Data Size')\n",
        "plt.ylabel('Execution Time (seconds)')\n",
        "plt.title('Query Performance Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xscale('log')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        },
        "id": "QeMkzXucJ8xj",
        "outputId": "cea67807-050a-4a00-d930-c15dd11e7cf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Query - GroupBy Implementation"
      ],
      "metadata": {
        "id": "9dQtiLsBJ_lX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile db_engine/advanced_query.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <pthread.h>\n",
        "\n",
        "#define MAX_THREADS 16\n",
        "#define MAX_GROUPS 100\n",
        "\n",
        "// Thread data structure for CPU groupby\n",
        "typedef struct {\n",
        "    int* values;\n",
        "    int* keys;\n",
        "    int start;\n",
        "    int end;\n",
        "    int* group_sums;\n",
        "    int* group_counts;\n",
        "    pthread_mutex_t* mutex;\n",
        "} groupby_thread_data_t;\n",
        "\n",
        "// CPU worker function for groupby\n",
        "void* groupby_operation_cpu(void* arg) {\n",
        "    groupby_thread_data_t* thread_data = (groupby_thread_data_t*)arg;\n",
        "\n",
        "    // Local group counters\n",
        "    int local_sums[MAX_GROUPS] = {0};\n",
        "    int local_counts[MAX_GROUPS] = {0};\n",
        "\n",
        "    // Process the assigned chunk of data\n",
        "    for (int i = thread_data->start; i < thread_data->end; i++) {\n",
        "        int key = thread_data->keys[i];\n",
        "        if (key >= 0 && key < MAX_GROUPS) {\n",
        "            local_sums[key] += thread_data->values[i];\n",
        "            local_counts[key]++;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Critical section - update shared results\n",
        "    pthread_mutex_lock(thread_data->mutex);\n",
        "    for (int i = 0; i < MAX_GROUPS; i++) {\n",
        "        thread_data->group_sums[i] += local_sums[i];\n",
        "        thread_data->group_counts[i] += local_counts[i];\n",
        "    }\n",
        "    pthread_mutex_unlock(thread_data->mutex);\n",
        "\n",
        "    return NULL;\n",
        "}\n",
        "\n",
        "// CUDA kernel for groupby operation\n",
        "__global__ void groupby_kernel_gpu(int* values, int* keys, int data_size,\n",
        "                                 int* group_sums, int* group_counts) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (idx < data_size) {\n",
        "        int key = keys[idx];\n",
        "        if (key >= 0 && key < MAX_GROUPS) {\n",
        "            atomicAdd(&group_sums[key], values[idx]);\n",
        "            atomicAdd(&group_counts[key], 1);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// Main function that executes hybrid groupby query\n",
        "extern \"C\" void execute_hybrid_groupby(int* values, int* keys, int data_size,\n",
        "                                     int* group_sums, int* group_counts,\n",
        "                                     int cpu_threads, float gpu_ratio) {\n",
        "\n",
        "    // Determine amount of work for GPU and CPU\n",
        "    int gpu_size = (int)(data_size * gpu_ratio);\n",
        "    int cpu_size = data_size - gpu_size;\n",
        "\n",
        "    // Initialize group sums and counts to zero\n",
        "    memset(group_sums, 0, sizeof(int) * MAX_GROUPS);\n",
        "    memset(group_counts, 0, sizeof(int) * MAX_GROUPS);\n",
        "\n",
        "    // ===== GPU PORTION =====\n",
        "    if (gpu_size > 0) {\n",
        "        int *d_values, *d_keys, *d_group_sums, *d_group_counts;\n",
        "\n",
        "        // Allocate memory on GPU\n",
        "        cudaMalloc((void**)&d_values, sizeof(int) * gpu_size);\n",
        "        cudaMalloc((void**)&d_keys, sizeof(int) * gpu_size);\n",
        "        cudaMalloc((void**)&d_group_sums, sizeof(int) * MAX_GROUPS);\n",
        "        cudaMalloc((void**)&d_group_counts, sizeof(int) * MAX_GROUPS);\n",
        "\n",
        "        // Copy data to GPU\n",
        "        cudaMemcpy(d_values, values, sizeof(int) * gpu_size, cudaMemcpyHostToDevice);\n",
        "        cudaMemcpy(d_keys, keys, sizeof(int) * gpu_size, cudaMemcpyHostToDevice);\n",
        "        cudaMemset(d_group_sums, 0, sizeof(int) * MAX_GROUPS);\n",
        "        cudaMemset(d_group_counts, 0, sizeof(int) * MAX_GROUPS);\n",
        "\n",
        "        // Configure kernel execution\n",
        "        int block_size = 256;\n",
        "        int grid_size = (gpu_size + block_size - 1) / block_size;\n",
        "\n",
        "        // Execute GPU kernel\n",
        "        groupby_kernel_gpu<<<grid_size, block_size>>>(d_values, d_keys, gpu_size,\n",
        "                                                  d_group_sums, d_group_counts);\n",
        "\n",
        "        // Copy results back to host\n",
        "        int* gpu_group_sums = (int*)malloc(sizeof(int) * MAX_GROUPS);\n",
        "        int* gpu_group_counts = (int*)malloc(sizeof(int) * MAX_GROUPS);\n",
        "\n",
        "        cudaMemcpy(gpu_group_sums, d_group_sums, sizeof(int) * MAX_GROUPS, cudaMemcpyDeviceToHost);\n",
        "        cudaMemcpy(gpu_group_counts, d_group_counts, sizeof(int) * MAX_GROUPS, cudaMemcpyDeviceToHost);\n",
        "\n",
        "        // Add GPU results to final results\n",
        "        for (int i = 0; i < MAX_GROUPS; i++) {\n",
        "            group_sums[i] += gpu_group_sums[i];\n",
        "            group_counts[i] += gpu_group_counts[i];\n",
        "        }\n",
        "\n",
        "        // Free GPU memory\n",
        "        cudaFree(d_values);\n",
        "        cudaFree(d_keys);\n",
        "        cudaFree(d_group_sums);\n",
        "        cudaFree(d_group_counts);\n",
        "        free(gpu_group_sums);\n",
        "        free(gpu_group_counts);\n",
        "    }\n",
        "\n",
        "    // ===== CPU PORTION =====\n",
        "    if (cpu_size > 0) {\n",
        "        pthread_t threads[MAX_THREADS];\n",
        "        groupby_thread_data_t thread_data[MAX_THREADS];\n",
        "        pthread_mutex_t mutex;\n",
        "\n",
        "        // Initialize mutex\n",
        "        pthread_mutex_init(&mutex, NULL);\n",
        "\n",
        "        // Calculate chunk size for each CPU thread\n",
        "        int chunk_size = cpu_size / cpu_threads;\n",
        "\n",
        "        // Create CPU threads\n",
        "        for (int i = 0; i < cpu_threads; i++) {\n",
        "            thread_data[i].values = values + gpu_size; // Start after GPU portion\n",
        "            thread_data[i].keys = keys + gpu_size;\n",
        "            thread_data[i].start = i * chunk_size;\n",
        "            thread_data[i].end = (i == cpu_threads - 1) ? cpu_size : (i + 1) * chunk_size;\n",
        "            thread_data[i].group_sums = group_sums;\n",
        "            thread_data[i].group_counts = group_counts;\n",
        "            thread_data[i].mutex = &mutex;\n",
        "\n",
        "            pthread_create(&threads[i], NULL, groupby_operation_cpu, &thread_data[i]);\n",
        "        }\n",
        "\n",
        "        // Join CPU threads\n",
        "        for (int i = 0; i < cpu_threads; i++) {\n",
        "            pthread_join(threads[i], NULL);\n",
        "        }\n",
        "\n",
        "        // Destroy mutex\n",
        "        pthread_mutex_destroy(&mutex);\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m-1Vkx6KJee",
        "outputId": "9e445cee-eedd-4f12-85fd-19c32cf53d22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing db_engine/advanced_query.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compile the Advanced Query Code"
      ],
      "metadata": {
        "id": "FRkuWSdbKRQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, check if the directory exists, create it if needed\n",
        "!mkdir -p db_engine\n",
        "\n",
        "# Compile the advanced query code with -fPIC flag\n",
        "!cd db_engine && nvcc -c advanced_query.cu -o advanced_query.o -Xcompiler -fPIC\n",
        "\n",
        "# Create the shared library with -fPIC flag\n",
        "!cd db_engine && nvcc -shared -o libadvanced_query.so advanced_query.o -lpthread -Xcompiler -fPIC\n",
        "\n",
        "# Verify the library was created\n",
        "!ls -l db_engine/\n",
        "\n",
        "# Now load the library\n",
        "import ctypes\n",
        "import numpy as np\n",
        "\n",
        "# Load the advanced query library\n",
        "advanced_lib = ctypes.CDLL('./db_engine/libadvanced_query.so')\n",
        "\n",
        "# Define argument types for the groupby function\n",
        "advanced_lib.execute_hybrid_groupby.argtypes = [\n",
        "    np.ctypeslib.ndpointer(dtype=np.int32, ndim=1, flags='C_CONTIGUOUS'),\n",
        "    np.ctypeslib.ndpointer(dtype=np.int32, ndim=1, flags='C_CONTIGUOUS'),\n",
        "    ctypes.c_int,\n",
        "    np.ctypeslib.ndpointer(dtype=np.int32, ndim=1, flags='C_CONTIGUOUS'),\n",
        "    np.ctypeslib.ndpointer(dtype=np.int32, ndim=1, flags='C_CONTIGUOUS'),\n",
        "    ctypes.c_int,\n",
        "    ctypes.c_float\n",
        "]\n",
        "advanced_lib.execute_hybrid_groupby.restype = None\n",
        "\n",
        "print(\"Library successfully loaded!\")"
      ],
      "metadata": {
        "id": "McCMSC97KSWr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1b6322e-e6d0-468e-93ee-e5cc7f41b7da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 3040\n",
            "-rw-r--r-- 1 root root    5305 May  4 09:48 advanced_query.cu\n",
            "-rw-r--r-- 1 root root   13144 May  4 09:48 advanced_query.o\n",
            "-rw-r--r-- 1 root root    1959 May  4 09:47 cuda_query.cu\n",
            "-rw-r--r-- 1 root root   10400 May  4 09:47 cuda_query.o\n",
            "-rw-r--r-- 1 root root    4760 May  4 09:47 hybrid_query.cu\n",
            "-rw-r--r-- 1 root root   12408 May  4 09:48 hybrid_query.o\n",
            "-rwxr-xr-x 1 root root 1004112 May  4 09:48 libadvanced_query.so\n",
            "-rwxr-xr-x 1 root root 1004032 May  4 09:48 libcuda_query.so\n",
            "-rwxr-xr-x 1 root root 1004096 May  4 09:48 libhybrid_query.so\n",
            "-rwxr-xr-x 1 root root   16104 May  4 09:48 libpthread_query.so\n",
            "-rw-r--r-- 1 root root    2448 May  4 09:47 pthread_query.cpp\n",
            "-rw-r--r-- 1 root root    3072 May  4 09:48 pthread_query.o\n",
            "Library successfully loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add these imports at the top\n",
        "import numpy as np\n",
        "import time\n",
        "import ctypes\n",
        "\n",
        "# Load the advanced query library (must be done before using advanced_lib)\n",
        "advanced_lib = ctypes.CDLL('./db_engine/libadvanced_query.so')\n",
        "\n",
        "# Define argument types for the groupby function\n",
        "advanced_lib.execute_hybrid_groupby.argtypes = [\n",
        "    np.ctypeslib.ndpointer(dtype=np.int32, ndim=1, flags='C_CONTIGUOUS'),\n",
        "    np.ctypeslib.ndpointer(dtype=np.int32, ndim=1, flags='C_CONTIGUOUS'),\n",
        "    ctypes.c_int,\n",
        "    np.ctypeslib.ndpointer(dtype=np.int32, ndim=1, flags='C_CONTIGUOUS'),\n",
        "    np.ctypeslib.ndpointer(dtype=np.int32, ndim=1, flags='C_CONTIGUOUS'),\n",
        "    ctypes.c_int,\n",
        "    ctypes.c_float\n",
        "]\n",
        "advanced_lib.execute_hybrid_groupby.restype = None"
      ],
      "metadata": {
        "id": "L_WZb_gKP9Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the GroupBy Operation"
      ],
      "metadata": {
        "id": "EWMivil6KVpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to test groupby operation\n",
        "def test_groupby(values, keys, num_groups, cpu_threads, gpu_ratio):\n",
        "    group_sums = np.zeros(num_groups, dtype=np.int32)\n",
        "    group_counts = np.zeros(num_groups, dtype=np.int32)\n",
        "\n",
        "    start_time = time.time()\n",
        "    advanced_lib.execute_hybrid_groupby(values, keys, len(values), group_sums, group_counts, cpu_threads, gpu_ratio)\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate averages for each group\n",
        "    group_avgs = np.zeros(num_groups, dtype=np.float32)\n",
        "    for i in range(num_groups):\n",
        "        if group_counts[i] > 0:\n",
        "            group_avgs[i] = group_sums[i] / group_counts[i]\n",
        "\n",
        "    return group_sums, group_counts, group_avgs, end_time - start_time\n",
        "\n",
        "# Generate test data for groupby operation\n",
        "data_size = 2000000\n",
        "num_groups = 10\n",
        "values = np.random.randint(0, 100, size=data_size, dtype=np.int32)\n",
        "keys = np.random.randint(0, num_groups, size=data_size, dtype=np.int32)\n",
        "\n",
        "# Test groupby with different GPU/CPU ratios\n",
        "print(\"\\nTesting GroupBy operation with different GPU/CPU ratios:\")\n",
        "for ratio in [0.0, 0.25, 0.5, 0.75, 1.0]:\n",
        "    group_sums, group_counts, group_avgs, execution_time = test_groupby(values, keys, num_groups, 4, ratio)\n",
        "    print(f\"GPU ratio {ratio:.2f} - Execution time: {execution_time:.4f} seconds\")\n",
        "\n",
        "    # Print results for first few groups\n",
        "    print(\"Group results (first 5 groups):\")\n",
        "    for i in range(min(5, num_groups)):\n",
        "        print(f\"  Group {i}: Sum = {group_sums[i]}, Count = {group_counts[i]}, Avg = {group_avgs[i]:.2f}\")\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "zHaqn5jcKZDo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f7ac2e3-ff78-42f3-8c65-735c00a864d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing GroupBy operation with different GPU/CPU ratios:\n",
            "GPU ratio 0.00 - Execution time: 0.0098 seconds\n",
            "Group results (first 5 groups):\n",
            "  Group 0: Sum = 9889856, Count = 200156, Avg = 49.41\n",
            "  Group 1: Sum = 9902499, Count = 200170, Avg = 49.47\n",
            "  Group 2: Sum = 9901188, Count = 199823, Avg = 49.55\n",
            "  Group 3: Sum = 9871747, Count = 199650, Avg = 49.45\n",
            "  Group 4: Sum = 9903211, Count = 200166, Avg = 49.47\n",
            "\n",
            "GPU ratio 0.25 - Execution time: 0.2126 seconds\n",
            "Group results (first 5 groups):\n",
            "  Group 0: Sum = 7413698, Count = 150157, Avg = 49.37\n",
            "  Group 1: Sum = 7420918, Count = 150097, Avg = 49.44\n",
            "  Group 2: Sum = 7419769, Count = 149686, Avg = 49.57\n",
            "  Group 3: Sum = 7394370, Count = 149539, Avg = 49.45\n",
            "  Group 4: Sum = 7424073, Count = 150092, Avg = 49.46\n",
            "\n",
            "GPU ratio 0.50 - Execution time: 0.0078 seconds\n",
            "Group results (first 5 groups):\n",
            "  Group 0: Sum = 4918339, Count = 99949, Avg = 49.21\n",
            "  Group 1: Sum = 4949517, Count = 100081, Avg = 49.46\n",
            "  Group 2: Sum = 4947734, Count = 99769, Avg = 49.59\n",
            "  Group 3: Sum = 4915614, Count = 99437, Avg = 49.43\n",
            "  Group 4: Sum = 4951853, Count = 100106, Avg = 49.47\n",
            "\n",
            "GPU ratio 0.75 - Execution time: 0.0061 seconds\n",
            "Group results (first 5 groups):\n",
            "  Group 0: Sum = 2455118, Count = 49979, Avg = 49.12\n",
            "  Group 1: Sum = 2474419, Count = 50038, Avg = 49.45\n",
            "  Group 2: Sum = 2459521, Count = 49710, Avg = 49.48\n",
            "  Group 3: Sum = 2459527, Count = 49761, Avg = 49.43\n",
            "  Group 4: Sum = 2465716, Count = 49985, Avg = 49.33\n",
            "\n",
            "GPU ratio 1.00 - Execution time: 0.0046 seconds\n",
            "Group results (first 5 groups):\n",
            "  Group 0: Sum = 0, Count = 0, Avg = 0.00\n",
            "  Group 1: Sum = 0, Count = 0, Avg = 0.00\n",
            "  Group 2: Sum = 0, Count = 0, Avg = 0.00\n",
            "  Group 3: Sum = 0, Count = 0, Avg = 0.00\n",
            "  Group 4: Sum = 0, Count = 0, Avg = 0.00\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code for Join Operations"
      ],
      "metadata": {
        "id": "_uUeXSCbtrXv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Wgpbwh8wv6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define HASH_TABLE_SIZE 1048576  // 2^20, can be adjusted based on expected data size\n",
        "\n",
        "extern \"C\" {\n",
        "    // GPU kernel for join operation using hash-based approach\n",
        "    __global__ void join_kernel_gpu(int* left_keys, int* left_values, int left_size,\n",
        "                                 int* right_keys, int* right_values, int right_size,\n",
        "                                 int* output_keys, int* output_left_values, int* output_right_values,\n",
        "                                 int* output_counter, int join_type) {\n",
        "        // Get the global thread ID\n",
        "        int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "        if (idx < left_size) {\n",
        "            int left_key = left_keys[idx];\n",
        "            int left_value = left_values[idx];\n",
        "            bool match_found = false;\n",
        "\n",
        "            // Iterate through right table to find matches (naive approach)\n",
        "            // In a real implementation, you'd use a hash table or other efficient data structure\n",
        "            for (int j = 0; j < right_size; j++) {\n",
        "                if (right_keys[j] == left_key) {\n",
        "                    // We found a match\n",
        "                    match_found = true;\n",
        "\n",
        "                    // Atomically get the next output position\n",
        "                    int output_pos = atomicAdd(output_counter, 1);\n",
        "\n",
        "                    // Write the output record\n",
        "                    output_keys[output_pos] = left_key;\n",
        "                    output_left_values[output_pos] = left_value;\n",
        "                    output_right_values[output_pos] = right_values[j];\n",
        "                }\n",
        "            }\n",
        "\n",
        "            // For LEFT JOIN or FULL JOIN, we need to output NULL for non-matching rows\n",
        "            if (!match_found && (join_type == 1 || join_type == 3)) {  // LEFT or FULL JOIN\n",
        "                int output_pos = atomicAdd(output_counter, 1);\n",
        "                output_keys[output_pos] = left_key;\n",
        "                output_left_values[output_pos] = left_value;\n",
        "                output_right_values[output_pos] = -1;  // Use -1 to represent NULL\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // For RIGHT JOIN or FULL JOIN, handle right table rows that didn't match any left table rows\n",
        "        // This would typically be handled in a separate kernel or CPU code for simplicity\n",
        "    }\n",
        "\n",
        "    // Main hybrid function that can be called from Python\n",
        "    void execute_hybrid_join(int* left_keys, int* left_values, int left_size,\n",
        "                          int* right_keys, int* right_values, int right_size,\n",
        "                          int* output_keys, int* output_left_values, int* output_right_values,\n",
        "                          int* output_size, int join_type, int cpu_threads, float gpu_ratio) {\n",
        "\n",
        "        // Determine how much work to do on GPU vs CPU\n",
        "        int gpu_elements = (int)(left_size * gpu_ratio);\n",
        "        int cpu_elements = left_size - gpu_elements;\n",
        "\n",
        "        // Initialize output size\n",
        "        *output_size = 0;\n",
        "\n",
        "        // Process GPU portion\n",
        "        if (gpu_elements > 0) {\n",
        "            int threadsPerBlock = 256;\n",
        "            int blocksPerGrid = (gpu_elements + threadsPerBlock - 1) / threadsPerBlock;\n",
        "\n",
        "            // Allocate device memory\n",
        "            int* d_left_keys;\n",
        "            int* d_left_values;\n",
        "            int* d_right_keys;\n",
        "            int* d_right_values;\n",
        "            int* d_output_keys;\n",
        "            int* d_output_left_values;\n",
        "            int* d_output_right_values;\n",
        "            int* d_output_counter;\n",
        "\n",
        "            // Calculate maximum possible output size (worst case: every left row matches every right row)\n",
        "            int max_output_size = left_size * right_size;\n",
        "\n",
        "            cudaMalloc((void**)&d_left_keys, gpu_elements * sizeof(int));\n",
        "            cudaMalloc((void**)&d_left_values, gpu_elements * sizeof(int));\n",
        "            cudaMalloc((void**)&d_right_keys, right_size * sizeof(int));\n",
        "            cudaMalloc((void**)&d_right_values, right_size * sizeof(int));\n",
        "            cudaMalloc((void**)&d_output_keys, max_output_size * sizeof(int));\n",
        "            cudaMalloc((void**)&d_output_left_values, max_output_size * sizeof(int));\n",
        "            cudaMalloc((void**)&d_output_right_values, max_output_size * sizeof(int));\n",
        "            cudaMalloc((void**)&d_output_counter, sizeof(int));\n",
        "\n",
        "            // Initialize output counter to 0\n",
        "            cudaMemset(d_output_counter, 0, sizeof(int));\n",
        "\n",
        "            // Copy data to device\n",
        "            cudaMemcpy(d_left_keys, left_keys, gpu_elements * sizeof(int), cudaMemcpyHostToDevice);\n",
        "            cudaMemcpy(d_left_values, left_values, gpu_elements * sizeof(int), cudaMemcpyHostToDevice);\n",
        "            cudaMemcpy(d_right_keys, right_keys, right_size * sizeof(int), cudaMemcpyHostToDevice);\n",
        "            cudaMemcpy(d_right_values, right_values, right_size * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "            // Launch kernel\n",
        "            join_kernel_gpu<<<blocksPerGrid, threadsPerBlock>>>(\n",
        "                d_left_keys, d_left_values, gpu_elements,\n",
        "                d_right_keys, d_right_values, right_size,\n",
        "                d_output_keys, d_output_left_values, d_output_right_values,\n",
        "                d_output_counter, join_type\n",
        "            );\n",
        "\n",
        "            // Copy output counter back to host\n",
        "            int gpu_output_count;\n",
        "            cudaMemcpy(&gpu_output_count, d_output_counter, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "            // Copy results back to host\n",
        "            cudaMemcpy(output_keys, d_output_keys, gpu_output_count * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "            cudaMemcpy(output_left_values, d_output_left_values, gpu_output_count * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "            cudaMemcpy(output_right_values, d_output_right_values, gpu_output_count * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "            // Update output size\n",
        "            *output_size = gpu_output_count;\n",
        "\n",
        "            // Free device memory\n",
        "            cudaFree(d_left_keys);\n",
        "            cudaFree(d_left_values);\n",
        "            cudaFree(d_right_keys);\n",
        "            cudaFree(d_right_values);\n",
        "            cudaFree(d_output_keys);\n",
        "            cudaFree(d_output_left_values);\n",
        "            cudaFree(d_output_right_values);\n",
        "            cudaFree(d_output_counter);\n",
        "        }\n",
        "\n",
        "        // Process CPU portion (simplified approach)\n",
        "        if (cpu_elements > 0) {\n",
        "            int cpu_output_start = *output_size;\n",
        "            int cpu_output_count = 0;\n",
        "\n",
        "            // Process each row from left table starting from gpu_elements\n",
        "            for (int i = gpu_elements; i < left_size; i++) {\n",
        "                int left_key = left_keys[i];\n",
        "                int left_value = left_values[i];\n",
        "                bool match_found = false;\n",
        "\n",
        "                // Find matches in right table\n",
        "                for (int j = 0; j < right_size; j++) {\n",
        "                    if (right_keys[j] == left_key) {\n",
        "                        // We found a match\n",
        "                        match_found = true;\n",
        "\n",
        "                        // Write the output record\n",
        "                        output_keys[cpu_output_start + cpu_output_count] = left_key;\n",
        "                        output_left_values[cpu_output_start + cpu_output_count] = left_value;\n",
        "                        output_right_values[cpu_output_start + cpu_output_count] = right_values[j];\n",
        "                        cpu_output_count++;\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                // For LEFT JOIN or FULL JOIN, we need to output NULL for non-matching rows\n",
        "                if (!match_found && (join_type == 1 || join_type == 3)) {  // LEFT or FULL JOIN\n",
        "                    output_keys[cpu_output_start + cpu_output_count] = left_key;\n",
        "                    output_left_values[cpu_output_start + cpu_output_count] = left_value;\n",
        "                    output_right_values[cpu_output_start + cpu_output_count] = -1;  // Use -1 to represent NULL\n",
        "                    cpu_output_count++;\n",
        "                }\n",
        "            }\n",
        "\n",
        "            // Handle RIGHT JOIN and FULL JOIN for unmatched right rows\n",
        "            if (join_type == 2 || join_type == 3) {  // RIGHT or FULL JOIN\n",
        "                for (int j = 0; j < right_size; j++) {\n",
        "                    int right_key = right_keys[j];\n",
        "                    bool match_found = false;\n",
        "\n",
        "                    // Check if this right row has a match in left table\n",
        "                    for (int i = 0; i < left_size; i++) {\n",
        "                        if (left_keys[i] == right_key) {\n",
        "                            match_found = true;\n",
        "                            break;\n",
        "                        }\n",
        "                    }\n",
        "\n",
        "                    // If no match found, add to output with NULL for left values\n",
        "                    if (!match_found) {\n",
        "                        output_keys[cpu_output_start + cpu_output_count] = right_key;\n",
        "                        output_left_values[cpu_output_start + cpu_output_count] = -1;  // Use -1 to represent NULL\n",
        "                        output_right_values[cpu_output_start + cpu_output_count] = right_values[j];\n",
        "                        cpu_output_count++;\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "\n",
        "            // Update output size\n",
        "            *output_size += cpu_output_count;\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "TAa_XrX1twAg",
        "outputId": "cb978a17-920f-40aa-d9dd-9ff6adc464ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 22) (<ipython-input-23-ff8ff9bfaa77>, line 22)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-ff8ff9bfaa77>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    // In a real implementation, you'd use a hash table or other efficient data structure\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the JOIN Operations"
      ],
      "metadata": {
        "id": "S-oLJt7oSawm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import ctypes\n",
        "import os\n",
        "\n",
        "# First, check if the directory exists, create it if needed\n",
        "!mkdir -p db_engine\n",
        "\n",
        "# Compile the join query code with -fPIC flag\n",
        "!cd db_engine && nvcc -c advanced_query_join.cu -o advanced_query_join.o -Xcompiler -fPIC\n",
        "\n",
        "# Create the shared library with -fPIC flag\n",
        "!cd db_engine && nvcc -shared -o libjoin_query.so advanced_query_join.o -lpthread -Xcompiler -fPIC\n",
        "\n",
        "# Verify the library was created\n",
        "!ls -l db_engine/\n",
        "\n",
        "# Now load the library\n",
        "join_lib = ctypes.CDLL('./db_engine/libjoin_query.so')\n",
        "\n",
        "# Define argument types for the join function\n",
        "join_lib.execute_hybrid_join.argtypes = [\n",
        "    np.ctypeslib.ndpointer(dtype=np.int32, ndim=1, flags='C_CONTIGUOUS'),  # left keys\n",
        "    np.ctypeslib.ndpointer(dtype=np.int32, ndim=1, flags='C_CONTIGUOUS'),  # left values\n",
        "    ctypes.c_int,  # left size\n",
        "    np.ctypeslib.ndpointer(dtype=np.int32, ndim=1, flags='C_CONTIGUOUS'),  # right keys\n",
        "    np.ctypeslib.ndpointer(dtype=np.int32, ndim=1, flags='C_CONTIGUOUS'),  # right values\n",
        "    ctypes.c_int,  # right size\n",
        "    np.ctypeslib.ndpointer(dtype=np.int32, ndim=1, flags='C_CONTIGUOUS'),  # output keys\n",
        "    np.ctypeslib.ndpointer(dtype=np.int32, ndim=1, flags='C_CONTIGUOUS'),  # output left values\n",
        "    np.ctypeslib.ndpointer(dtype=np.int32, ndim=1, flags='C_CONTIGUOUS'),  # output right values\n",
        "    ctypes.POINTER(ctypes.c_int),  # output size\n",
        "    ctypes.c_int,  # join type (0: inner, 1: left, 2: right, 3: full)\n",
        "    ctypes.c_int,  # cpu threads\n",
        "    ctypes.c_float  # gpu ratio\n",
        "]\n",
        "join_lib.execute_hybrid_join.restype = None\n",
        "\n",
        "print(\"Join library successfully loaded!\")\n",
        "\n",
        "# Function to test join operation\n",
        "def test_join(left_keys, left_values, right_keys, right_values, join_type, cpu_threads, gpu_ratio):\n",
        "    # Calculate maximum possible output size (worst case: every left row matches every right row)\n",
        "    max_output_size = len(left_keys) * len(right_keys)\n",
        "\n",
        "    # Prepare output arrays\n",
        "    output_keys = np.zeros(max_output_size, dtype=np.int32)\n",
        "    output_left_values = np.zeros(max_output_size, dtype=np.int32)\n",
        "    output_right_values = np.zeros(max_output_size, dtype=np.int32)\n",
        "    output_size = ctypes.c_int(0)\n",
        "\n",
        "    # Measure execution time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Call the join function\n",
        "    join_lib.execute_hybrid_join(\n",
        "        left_keys, left_values, len(left_keys),\n",
        "        right_keys, right_values, len(right_keys),\n",
        "        output_keys, output_left_values, output_right_values,\n",
        "        ctypes.byref(output_size), join_type, cpu_threads, gpu_ratio\n",
        "    )\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Extract actual results\n",
        "    actual_size = output_size.value\n",
        "    result_keys = output_keys[:actual_size].copy()\n",
        "    result_left_values = output_left_values[:actual_size].copy()\n",
        "    result_right_values = output_right_values[:actual_size].copy()\n",
        "\n",
        "    return result_keys, result_left_values, result_right_values, end_time - start_time, actual_size\n",
        "\n",
        "# Generate test data for join operation\n",
        "# Left table\n",
        "left_size = 100000\n",
        "left_keys = np.random.randint(1, 1000, size=left_size, dtype=np.int32)\n",
        "left_values = np.random.randint(0, 10000, size=left_size, dtype=np.int32)\n",
        "\n",
        "# Right table\n",
        "right_size = 50000\n",
        "right_keys = np.random.randint(1, 1500, size=right_size, dtype=np.int32)\n",
        "right_values = np.random.randint(0, 10000, size=right_size, dtype=np.int32)\n",
        "\n",
        "# Join types\n",
        "join_types = {\n",
        "    0: \"INNER JOIN\",\n",
        "    1: \"LEFT JOIN\",\n",
        "    2: \"RIGHT JOIN\",\n",
        "    3: \"FULL JOIN\"\n",
        "}\n",
        "\n",
        "# Test join with different GPU/CPU ratios and join types\n",
        "print(\"\\nTesting Join operation with different GPU/CPU ratios:\")\n",
        "for join_type in [0, 1, 2, 3]:\n",
        "    print(f\"\\n---- {join_types[join_type]} ----\")\n",
        "    for ratio in [0.0, 0.5, 1.0]:\n",
        "        result_keys, result_left_values, result_right_values, execution_time, result_count = test_join(\n",
        "            left_keys, left_values, right_keys, right_values, join_type, 4, ratio\n",
        "        )\n",
        "\n",
        "        print(f\"GPU ratio {ratio:.2f} - Execution time: {execution_time:.4f} seconds\")\n",
        "        print(f\"Result count: {result_count} rows\")\n",
        "\n",
        "        # Show sample of results (first 5 rows)\n",
        "        if result_count > 0:\n",
        "            print(\"Sample results (first 5 rows):\")\n",
        "            for i in range(min(5, result_count)):\n",
        "                left_val = \"NULL\" if result_left_values[i] == -1 else result_left_values[i]\n",
        "                right_val = \"NULL\" if result_right_values[i] == -1 else result_right_values[i]\n",
        "                print(f\"  Key: {result_keys[i]}, Left: {left_val}, Right: {right_val}\")\n",
        "        print(\"\")\n",
        "\n",
        "# Verify the correctness of join results (using INNER JOIN as an example)\n",
        "print(\"\\nVerifying the correctness of INNER JOIN results...\")\n",
        "# Perform the join in Python for comparison\n",
        "py_start_time = time.time()\n",
        "python_results = []\n",
        "for i in range(len(left_keys)):\n",
        "    for j in range(len(right_keys)):\n",
        "        if left_keys[i] == right_keys[j]:\n",
        "            python_results.append((left_keys[i], left_values[i], right_values[j]))\n",
        "py_end_time = time.time()\n",
        "\n",
        "# Run the GPU/CPU hybrid join with full GPU\n",
        "result_keys, result_left_values, result_right_values, execution_time, result_count = test_join(\n",
        "    left_keys, left_values, right_keys, right_values, 0, 4, 1.0\n",
        ")\n",
        "\n",
        "print(f\"Python pure implementation: {len(python_results)} results in {py_end_time - py_start_time:.4f} seconds\")\n",
        "print(f\"GPU/CPU hybrid implementation: {result_count} results in {execution_time:.4f} seconds\")\n",
        "print(f\"Speedup: {(py_end_time - py_start_time) / execution_time:.2f}x\")\n",
        "\n",
        "# If the result counts match, the implementations are likely correct\n",
        "print(f\"Results {'match' if len(python_results) == result_count else 'do not match'} in count\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "che8lL1YPqlM",
        "outputId": "ee971605-a7f7-4a47-f4c9-a5b78a269e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 1016\n",
            "-rw-r--r-- 1 root root    9329 May  5 18:46 advanced_query_join.cu\n",
            "-rw-r--r-- 1 root root   16120 May  5 18:50 advanced_query_join.o\n",
            "-rwxr-xr-x 1 root root 1008176 May  5 18:50 libjoin_query.so\n",
            "Join library successfully loaded!\n",
            "\n",
            "Testing Join operation with different GPU/CPU ratios:\n",
            "\n",
            "---- INNER JOIN ----\n",
            "GPU ratio 0.00 - Execution time: 15.6784 seconds\n",
            "Result count: 3332583 rows\n",
            "Sample results (first 5 rows):\n",
            "  Key: 845, Left: 8609, Right: 845\n",
            "  Key: 845, Left: 8609, Right: 2042\n",
            "  Key: 845, Left: 8609, Right: 6503\n",
            "  Key: 845, Left: 8609, Right: 9997\n",
            "  Key: 845, Left: 8609, Right: 4467\n",
            "\n",
            "GPU ratio 0.50 - Execution time: 6.7960 seconds\n",
            "Result count: 1666950 rows\n",
            "Sample results (first 5 rows):\n",
            "  Key: 554, Left: 8398, Right: 5011\n",
            "  Key: 554, Left: 8398, Right: 1410\n",
            "  Key: 554, Left: 8398, Right: 3612\n",
            "  Key: 554, Left: 8398, Right: 9334\n",
            "  Key: 554, Left: 8398, Right: 1434\n",
            "\n",
            "GPU ratio 1.00 - Execution time: 0.0119 seconds\n",
            "Result count: 0 rows\n",
            "\n",
            "\n",
            "---- LEFT JOIN ----\n",
            "GPU ratio 0.00 - Execution time: 12.9649 seconds\n",
            "Result count: 3332583 rows\n",
            "Sample results (first 5 rows):\n",
            "  Key: 845, Left: 8609, Right: 845\n",
            "  Key: 845, Left: 8609, Right: 2042\n",
            "  Key: 845, Left: 8609, Right: 6503\n",
            "  Key: 845, Left: 8609, Right: 9997\n",
            "  Key: 845, Left: 8609, Right: 4467\n",
            "\n",
            "GPU ratio 0.50 - Execution time: 6.4321 seconds\n",
            "Result count: 1666950 rows\n",
            "Sample results (first 5 rows):\n",
            "  Key: 554, Left: 8398, Right: 5011\n",
            "  Key: 554, Left: 8398, Right: 1410\n",
            "  Key: 554, Left: 8398, Right: 3612\n",
            "  Key: 554, Left: 8398, Right: 9334\n",
            "  Key: 554, Left: 8398, Right: 1434\n",
            "\n",
            "GPU ratio 1.00 - Execution time: 0.0119 seconds\n",
            "Result count: 0 rows\n",
            "\n",
            "\n",
            "---- RIGHT JOIN ----\n",
            "GPU ratio 0.00 - Execution time: 17.8273 seconds\n",
            "Result count: 3349288 rows\n",
            "Sample results (first 5 rows):\n",
            "  Key: 845, Left: 8609, Right: 845\n",
            "  Key: 845, Left: 8609, Right: 2042\n",
            "  Key: 845, Left: 8609, Right: 6503\n",
            "  Key: 845, Left: 8609, Right: 9997\n",
            "  Key: 845, Left: 8609, Right: 4467\n",
            "\n",
            "GPU ratio 0.50 - Execution time: 10.7767 seconds\n",
            "Result count: 1683655 rows\n",
            "Sample results (first 5 rows):\n",
            "  Key: 554, Left: 8398, Right: 5011\n",
            "  Key: 554, Left: 8398, Right: 1410\n",
            "  Key: 554, Left: 8398, Right: 3612\n",
            "  Key: 554, Left: 8398, Right: 9334\n",
            "  Key: 554, Left: 8398, Right: 1434\n",
            "\n",
            "GPU ratio 1.00 - Execution time: 0.0117 seconds\n",
            "Result count: 0 rows\n",
            "\n",
            "\n",
            "---- FULL JOIN ----\n",
            "GPU ratio 0.00 - Execution time: 17.1387 seconds\n",
            "Result count: 3349288 rows\n",
            "Sample results (first 5 rows):\n",
            "  Key: 845, Left: 8609, Right: 845\n",
            "  Key: 845, Left: 8609, Right: 2042\n",
            "  Key: 845, Left: 8609, Right: 6503\n",
            "  Key: 845, Left: 8609, Right: 9997\n",
            "  Key: 845, Left: 8609, Right: 4467\n",
            "\n",
            "GPU ratio 0.50 - Execution time: 10.7604 seconds\n",
            "Result count: 1683655 rows\n",
            "Sample results (first 5 rows):\n",
            "  Key: 554, Left: 8398, Right: 5011\n",
            "  Key: 554, Left: 8398, Right: 1410\n",
            "  Key: 554, Left: 8398, Right: 3612\n",
            "  Key: 554, Left: 8398, Right: 9334\n",
            "  Key: 554, Left: 8398, Right: 1434\n",
            "\n",
            "GPU ratio 1.00 - Execution time: 0.0119 seconds\n",
            "Result count: 0 rows\n",
            "\n",
            "\n",
            "Verifying the correctness of INNER JOIN results...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aggregate Functions"
      ],
      "metadata": {
        "id": "S5eNEF5eTWN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import multiprocessing\n",
        "from enum import IntEnum\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# First, let's create the C library for Linux (Colab runs on Linux)\n",
        "%%writefile advanced.c\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "#include <omp.h>\n",
        "\n",
        "// Enum matching the Python AggregateOperation\n",
        "typedef enum {\n",
        "    SUM = 0,\n",
        "    MIN = 1,\n",
        "    MAX = 2,\n",
        "    AVG = 3,\n",
        "    COUNT = 4\n",
        "} AggregateOperation;\n",
        "\n",
        "// Function to simulate CPU processing\n",
        "void cpu_aggregate(int* values, int start, int end, int* local_result, int operation) {\n",
        "    if (end <= start) return;\n",
        "\n",
        "    switch (operation) {\n",
        "        case SUM:\n",
        "        case AVG:\n",
        "            *local_result = 0;\n",
        "            for (int i = start; i < end; i++) {\n",
        "                *local_result += values[i];\n",
        "            }\n",
        "            break;\n",
        "        case MIN:\n",
        "            *local_result = values[start];\n",
        "            for (int i = start + 1; i < end; i++) {\n",
        "                if (values[i] < *local_result) {\n",
        "                    *local_result = values[i];\n",
        "                }\n",
        "            }\n",
        "            break;\n",
        "        case MAX:\n",
        "            *local_result = values[start];\n",
        "            for (int i = start + 1; i < end; i++) {\n",
        "                if (values[i] > *local_result) {\n",
        "                    *local_result = values[i];\n",
        "                }\n",
        "            }\n",
        "            break;\n",
        "        case COUNT:\n",
        "            *local_result = end - start;\n",
        "            break;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Function to simulate GPU processing (in this mock version, it's just CPU processing)\n",
        "void gpu_aggregate(int* values, int start, int end, int* local_result, int operation) {\n",
        "    // In a real implementation, this would use GPU acceleration\n",
        "    // For now, it's just a wrapper around CPU processing\n",
        "    cpu_aggregate(values, start, end, local_result, operation);\n",
        "}\n",
        "\n",
        "// Main function that will be called from Python\n",
        "void execute_hybrid_aggregate(int* values, int size, int* result, int operation, int cpu_threads, float gpu_ratio) {\n",
        "    // Calculate how much work to do on GPU vs CPU\n",
        "    int gpu_size = (int)(size * gpu_ratio);\n",
        "    int cpu_size = size - gpu_size;\n",
        "\n",
        "    // Store partial results\n",
        "    int gpu_result = 0;\n",
        "    int* cpu_results = (int*)malloc(cpu_threads * sizeof(int));\n",
        "\n",
        "    // Process GPU part (simulated)\n",
        "    if (gpu_size > 0) {\n",
        "        gpu_aggregate(values, 0, gpu_size, &gpu_result, operation);\n",
        "    }\n",
        "\n",
        "    // Process CPU part using OpenMP for parallelism\n",
        "    #pragma omp parallel num_threads(cpu_threads)\n",
        "    {\n",
        "        int thread_id = omp_get_thread_num();\n",
        "        int items_per_thread = cpu_size / cpu_threads;\n",
        "        int start = gpu_size + thread_id * items_per_thread;\n",
        "        int end = (thread_id == cpu_threads - 1) ? size : start + items_per_thread;\n",
        "\n",
        "        cpu_aggregate(values, start, end, &cpu_results[thread_id], operation);\n",
        "    }\n",
        "\n",
        "    // Combine results\n",
        "    *result = gpu_result;\n",
        "    for (int i = 0; i < cpu_threads; i++) {\n",
        "        switch (operation) {\n",
        "            case SUM:\n",
        "            case AVG:\n",
        "            case COUNT:\n",
        "                *result += cpu_results[i];\n",
        "                break;\n",
        "            case MIN:\n",
        "                if (i == 0 && gpu_size == 0) {\n",
        "                    *result = cpu_results[i];\n",
        "                } else if (cpu_results[i] < *result) {\n",
        "                    *result = cpu_results[i];\n",
        "                }\n",
        "                break;\n",
        "            case MAX:\n",
        "                if (i == 0 && gpu_size == 0) {\n",
        "                    *result = cpu_results[i];\n",
        "                } else if (cpu_results[i] > *result) {\n",
        "                    *result = cpu_results[i];\n",
        "                }\n",
        "                break;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // For average, divide the sum by the count\n",
        "    if (operation == AVG && size > 0) {\n",
        "        *result = *result / size;\n",
        "    }\n",
        "\n",
        "    free(cpu_results);\n",
        "}\n",
        "\n",
        "# Now compile the C library\n",
        "!gcc -shared -fPIC -fopenmp -o libadvanced.so advanced.c -lm\n",
        "\n",
        "# Define the AggregateOperation enum\n",
        "class AggregateOperation(IntEnum):\n",
        "    SUM = 0\n",
        "    MIN = 1\n",
        "    MAX = 2\n",
        "    AVG = 3\n",
        "    COUNT = 4\n",
        "\n",
        "\n",
        "class AdaptiveHybridAggregator:\n",
        "    \"\"\"\n",
        "    Adaptively adjusts CPU/GPU workload ratio based on data size, operation type,\n",
        "    and system characteristics to optimize performance.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lib_path='./libadvanced.so'):\n",
        "        \"\"\"\n",
        "        Initialize the aggregator with the C library.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        lib_path : str, optional\n",
        "            Path to the C library. Default is './libadvanced.so' for Colab.\n",
        "        \"\"\"\n",
        "        # Load the C library\n",
        "        import ctypes\n",
        "        try:\n",
        "            self.lib = ctypes.CDLL(lib_path)\n",
        "            print(f\"Successfully loaded library from {lib_path}\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to load library from {lib_path}: {e}\")\n",
        "\n",
        "        # Define function signature\n",
        "        self.lib.execute_hybrid_aggregate.argtypes = [\n",
        "            np.ctypeslib.ndpointer(dtype=np.int32, ndim=1, flags='C_CONTIGUOUS'),  # values\n",
        "            ctypes.c_int,  # size\n",
        "            ctypes.POINTER(ctypes.c_int),  # result\n",
        "            ctypes.c_int,  # operation type\n",
        "            ctypes.c_int,  # cpu threads\n",
        "            ctypes.c_float  # gpu ratio\n",
        "        ]\n",
        "\n",
        "        # Cache for optimal ratios\n",
        "        self._performance_cache = {}\n",
        "\n",
        "        # System info\n",
        "        self.cpu_count = multiprocessing.cpu_count()\n",
        "        import psutil\n",
        "        self.available_memory = psutil.virtual_memory().available\n",
        "\n",
        "        # Try to detect GPU info if possible\n",
        "        try:\n",
        "            # Check for GPU in Colab\n",
        "            import subprocess\n",
        "            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "            self.has_gpu = result.returncode == 0\n",
        "            if self.has_gpu:\n",
        "                print(\"GPU detected\")\n",
        "            else:\n",
        "                print(\"No GPU detected\")\n",
        "        except:\n",
        "            # Fallback assumption\n",
        "            self.has_gpu = False\n",
        "            print(\"Assuming no GPU is available\")\n",
        "\n",
        "    def _get_optimal_cpu_threads(self, data_size):\n",
        "        \"\"\"Calculate optimal number of CPU threads based on data size and system.\"\"\"\n",
        "        # Start with number of physical cores\n",
        "        optimal_threads = self.cpu_count\n",
        "\n",
        "        # For very small data, fewer threads to avoid overhead\n",
        "        if data_size < 1000:\n",
        "            optimal_threads = max(1, self.cpu_count // 4)\n",
        "        elif data_size < 10000:\n",
        "            optimal_threads = max(2, self.cpu_count // 2)\n",
        "\n",
        "        # Cap at reasonable maximum to avoid thread switching overhead\n",
        "        return min(optimal_threads, 16)\n",
        "\n",
        "    def _get_initial_gpu_ratio(self, data_size, operation):\n",
        "        \"\"\"Get initial GPU ratio based on data size and operation type.\"\"\"\n",
        "        # If no GPU, use CPU only\n",
        "        if not self.has_gpu:\n",
        "            return 0.0\n",
        "\n",
        "        # Operations that typically benefit more from GPU\n",
        "        gpu_friendly_ops = {AggregateOperation.SUM, AggregateOperation.COUNT}\n",
        "\n",
        "        # Base ratio - higher for GPU-friendly operations\n",
        "        base_ratio = 0.7 if operation in gpu_friendly_ops else 0.5\n",
        "\n",
        "        # Adjust based on data size\n",
        "        if data_size < 5000:  # Small data might not be worth GPU overhead\n",
        "            return base_ratio * 0.5\n",
        "        elif data_size > 1000000:  # Large data benefits more from GPU\n",
        "            return min(0.9, base_ratio * 1.5)\n",
        "\n",
        "        return base_ratio\n",
        "\n",
        "    def _benchmark_ratios(self, data, operation, cpu_threads, ratios_to_try=None):\n",
        "        \"\"\"Benchmark different GPU ratios to find optimal performance.\"\"\"\n",
        "        if ratios_to_try is None:\n",
        "            ratios_to_try = [0.0, 0.3, 0.5, 0.7, 0.9]\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        for ratio in ratios_to_try:\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Run 3 times to get more stable measurements\n",
        "            for _ in range(3):\n",
        "                self.run_aggregate(data, operation, cpu_threads, ratio)\n",
        "\n",
        "            avg_time = (time.time() - start_time) / 3\n",
        "            results[ratio] = avg_time\n",
        "\n",
        "        # Find ratio with minimum execution time\n",
        "        optimal_ratio = min(results, key=results.get)\n",
        "        return optimal_ratio, results\n",
        "\n",
        "    def run_aggregate(self, values, operation, cpu_threads=None, gpu_ratio=None):\n",
        "        \"\"\"\n",
        "        Run aggregate operation with specified or auto-determined parameters.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        values : np.ndarray\n",
        "            A 1D numpy array of int32 values to aggregate\n",
        "        operation : AggregateOperation\n",
        "            The aggregate operation to perform\n",
        "        cpu_threads : int, optional\n",
        "            Number of CPU threads to use (default: auto-determined)\n",
        "        gpu_ratio : float, optional\n",
        "            Ratio of work to offload to GPU (default: auto-determined)\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        int or float: The result of the aggregate operation\n",
        "        \"\"\"\n",
        "        import ctypes\n",
        "\n",
        "        # Ensure values is a contiguous 1D array of int32\n",
        "        if not isinstance(values, np.ndarray):\n",
        "            values = np.array(values, dtype=np.int32)\n",
        "        elif values.dtype != np.int32:\n",
        "            values = values.astype(np.int32)\n",
        "\n",
        "        if not values.flags['C_CONTIGUOUS']:\n",
        "            values = np.ascontiguousarray(values)\n",
        "\n",
        "        if values.ndim != 1:\n",
        "            raise ValueError(\"Input array must be 1-dimensional\")\n",
        "\n",
        "        # Auto-determine parameters if not specified\n",
        "        data_size = len(values)\n",
        "\n",
        "        if cpu_threads is None:\n",
        "            cpu_threads = self._get_optimal_cpu_threads(data_size)\n",
        "\n",
        "        if gpu_ratio is None:\n",
        "            # Check if we have a cached optimal ratio for similar workload\n",
        "            cache_key = (data_size // 1000, int(operation))\n",
        "            if cache_key in self._performance_cache:\n",
        "                gpu_ratio = self._performance_cache[cache_key]\n",
        "            else:\n",
        "                gpu_ratio = self._get_initial_gpu_ratio(data_size, operation)\n",
        "\n",
        "        # Validate parameters\n",
        "        if cpu_threads < 1:\n",
        "            raise ValueError(\"Number of CPU threads must be at least 1\")\n",
        "\n",
        "        if not 0.0 <= gpu_ratio <= 1.0:\n",
        "            raise ValueError(\"GPU ratio must be between 0.0 and 1.0\")\n",
        "\n",
        "        # Prepare result variable\n",
        "        result = ctypes.c_int(0)\n",
        "        result_ptr = ctypes.byref(result)\n",
        "\n",
        "        # Call the C function\n",
        "        self.lib.execute_hybrid_aggregate(\n",
        "            values,\n",
        "            data_size,\n",
        "            result_ptr,\n",
        "            int(operation),\n",
        "            cpu_threads,\n",
        "            gpu_ratio\n",
        "        )\n",
        "\n",
        "        # Return the result (convert to float for AVG operation)\n",
        "        if operation == AggregateOperation.AVG:\n",
        "            return float(result.value)\n",
        "        else:\n",
        "            return result.value\n",
        "\n",
        "    def find_optimal_parameters(self, data, operation, save_to_cache=True):\n",
        "        \"\"\"\n",
        "        Find optimal CPU/GPU parameters for this workload through benchmarking.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        data : np.ndarray\n",
        "            Sample data representative of the workload\n",
        "        operation : AggregateOperation\n",
        "            The operation to optimize for\n",
        "        save_to_cache : bool\n",
        "            Whether to save results to the performance cache\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        tuple: (optimal_threads, optimal_gpu_ratio)\n",
        "        \"\"\"\n",
        "        data_size = len(data)\n",
        "\n",
        "        # Test different thread counts\n",
        "        thread_options = [1, 2, 4, 8, max(1, self.cpu_count)]\n",
        "        thread_options = sorted(list(set(thread_options)))  # Remove duplicates\n",
        "\n",
        "        best_threads = None\n",
        "        best_ratio = None\n",
        "        best_time = float('inf')\n",
        "\n",
        "        for threads in thread_options:\n",
        "            # Get initial GPU ratio estimate\n",
        "            initial_ratio = self._get_initial_gpu_ratio(data_size, operation)\n",
        "\n",
        "            # Fine-tune around the initial ratio\n",
        "            test_ratios = [max(0.0, initial_ratio - 0.2),\n",
        "                          initial_ratio,\n",
        "                          min(1.0, initial_ratio + 0.2)]\n",
        "\n",
        "            optimal_ratio, timing_results = self._benchmark_ratios(\n",
        "                data, operation, threads, test_ratios)\n",
        "\n",
        "            if timing_results[optimal_ratio] < best_time:\n",
        "                best_time = timing_results[optimal_ratio]\n",
        "                best_threads = threads\n",
        "                best_ratio = optimal_ratio\n",
        "\n",
        "        # Save to cache for future use\n",
        "        if save_to_cache:\n",
        "            cache_key = (data_size // 1000, int(operation))\n",
        "            self._performance_cache[cache_key] = best_ratio\n",
        "\n",
        "        return best_threads, best_ratio\n",
        "\n",
        "    def visualize_performance(self, data, operation):\n",
        "        \"\"\"\n",
        "        Create a visualization of performance across different CPU/GPU ratios.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        data : np.ndarray\n",
        "            Data to use for benchmarking\n",
        "        operation : AggregateOperation\n",
        "            Operation to benchmark\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        None (displays a plot)\n",
        "        \"\"\"\n",
        "        threads = self._get_optimal_cpu_threads(len(data))\n",
        "\n",
        "        # Test more granular ratios\n",
        "        ratios = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "        times = []\n",
        "\n",
        "        for ratio in ratios:\n",
        "            start = time.time()\n",
        "            # Run multiple times for more stable results\n",
        "            for _ in range(3):\n",
        "                self.run_aggregate(data, operation, threads, ratio)\n",
        "            avg_time = (time.time() - start) / 3\n",
        "            times.append(avg_time)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(ratios, times, 'o-', linewidth=2)\n",
        "        plt.xlabel('GPU Ratio')\n",
        "        plt.ylabel('Execution Time (s)')\n",
        "        plt.title(f'Performance vs. GPU Ratio for {operation.name} Operation\\n'\n",
        "                 f'(Data size: {len(data)}, CPU threads: {threads})')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "        best_ratio = ratios[times.index(min(times))]\n",
        "        print(f\"Optimal GPU ratio: {best_ratio:.2f}\")\n",
        "        return best_ratio\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Make sure we have the required packages\n",
        "        import subprocess\n",
        "        subprocess.check_call(['pip', 'install', 'psutil'])\n",
        "\n",
        "        # Create the adaptive aggregator\n",
        "        aggregator = AdaptiveHybridAggregator()\n",
        "\n",
        "        # Generate some test data\n",
        "        small_data = np.random.randint(0, 100, size=1000, dtype=np.int32)\n",
        "        medium_data = np.random.randint(0, 100, size=100000, dtype=np.int32)\n",
        "        large_data = np.random.randint(0, 100, size=10000000, dtype=np.int32)\n",
        "\n",
        "        print(\"Testing with different data sizes:\")\n",
        "\n",
        "        # Small data example\n",
        "        print(\"\\nSmall data (1,000 elements):\")\n",
        "        t_start = time.time()\n",
        "        optimal_threads, optimal_ratio = aggregator.find_optimal_parameters(small_data, AggregateOperation.SUM)\n",
        "        print(f\"Optimal parameters - CPU threads: {optimal_threads}, GPU ratio: {optimal_ratio:.2f}\")\n",
        "        result = aggregator.run_aggregate(small_data, AggregateOperation.SUM)\n",
        "        print(f\"Sum result: {result}\")\n",
        "        print(f\"Time with optimal parameters: {time.time() - t_start:.4f}s\")\n",
        "\n",
        "        # Compare with CPU-only\n",
        "        t_start = time.time()\n",
        "        result_cpu = aggregator.run_aggregate(small_data, AggregateOperation.SUM,\n",
        "                                             aggregator.cpu_count, 0.0)\n",
        "        cpu_time = time.time() - t_start\n",
        "        print(f\"CPU-only time: {cpu_time:.4f}s\")\n",
        "\n",
        "        # Medium data\n",
        "        print(\"\\nMedium data (100,000 elements):\")\n",
        "        t_start = time.time()\n",
        "        optimal_threads, optimal_ratio = aggregator.find_optimal_parameters(medium_data, AggregateOperation.SUM)\n",
        "        print(f\"Optimal parameters - CPU threads: {optimal_threads}, GPU ratio: {optimal_ratio:.2f}\")\n",
        "        result = aggregator.run_aggregate(medium_data, AggregateOperation.SUM)\n",
        "        print(f\"Sum result: {result}\")\n",
        "        print(f\"Time with optimal parameters: {time.time() - t_start:.4f}s\")\n",
        "\n",
        "        # Large data - visualize performance\n",
        "        print(\"\\nLarge data (10,000,000 elements):\")\n",
        "        print(\"Visualizing performance across different GPU ratios...\")\n",
        "        best_ratio = aggregator.visualize_performance(large_data, AggregateOperation.SUM)\n",
        "\n",
        "        # Performance comparison between operations\n",
        "        print(\"\\nComparing performance of different operations:\")\n",
        "        for op in AggregateOperation:\n",
        "            t_start = time.time()\n",
        "            result = aggregator.run_aggregate(medium_data, op)\n",
        "            op_time = time.time() - t_start\n",
        "            print(f\"{op.name}: {result} (time: {op_time:.4f}s)\")\n",
        "\n",
        "        print(\"\\nAdaptive CPU/GPU ratio demo completed!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"Please ensure the library is properly loaded and accessible.\")"
      ],
      "metadata": {
        "id": "lo9R7Zh-UYIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a simpler fix that doesn't require the C library\n",
        "# It implements the functionality purely in Python\n",
        "\n",
        "import numpy as np\n",
        "import psutil\n",
        "import time\n",
        "import multiprocessing\n",
        "from enum import IntEnum\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class AggregateOperation(IntEnum):\n",
        "    SUM = 0\n",
        "    MIN = 1\n",
        "    MAX = 2\n",
        "    AVG = 3\n",
        "    COUNT = 4\n",
        "\n",
        "\n",
        "# Python implementation of the aggregate operations\n",
        "def python_aggregate(values, operation, cpu_threads=1, gpu_ratio=0.0):\n",
        "    \"\"\"Python implementation of aggregate operations without C library dependency\"\"\"\n",
        "    if operation == AggregateOperation.SUM:\n",
        "        return np.sum(values)\n",
        "    elif operation == AggregateOperation.MIN:\n",
        "        return np.min(values)\n",
        "    elif operation == AggregateOperation.MAX:\n",
        "        return np.max(values)\n",
        "    elif operation == AggregateOperation.AVG:\n",
        "        return np.mean(values)\n",
        "    elif operation == AggregateOperation.COUNT:\n",
        "        return len(values)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown operation: {operation}\")\n",
        "\n",
        "\n",
        "class AdaptiveHybridAggregator:\n",
        "    \"\"\"\n",
        "    Adaptively adjusts CPU/GPU workload ratio based on data size, operation type,\n",
        "    and system characteristics to optimize performance.\n",
        "    This is a Python-only implementation that simulates the C library behavior.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lib_path=None):\n",
        "        \"\"\"\n",
        "        Initialize the aggregator.\n",
        "        \"\"\"\n",
        "        print(\"Using pure Python implementation (no C library)\")\n",
        "\n",
        "        # Cache for optimal ratios\n",
        "        self._performance_cache = {}\n",
        "\n",
        "        # System info\n",
        "        self.cpu_count = multiprocessing.cpu_count()\n",
        "        self.available_memory = psutil.virtual_memory().available\n",
        "\n",
        "        # Try to detect GPU info if possible\n",
        "        try:\n",
        "            # Check for GPU in Colab\n",
        "            import subprocess\n",
        "            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "            self.has_gpu = result.returncode == 0\n",
        "            if self.has_gpu:\n",
        "                print(\"GPU detected\")\n",
        "            else:\n",
        "                print(\"No GPU detected\")\n",
        "        except:\n",
        "            # Fallback assumption\n",
        "            self.has_gpu = False\n",
        "            print(\"Assuming no GPU is available\")\n",
        "\n",
        "    def _get_optimal_cpu_threads(self, data_size):\n",
        "        \"\"\"Calculate optimal number of CPU threads based on data size and system.\"\"\"\n",
        "        # Start with number of physical cores\n",
        "        optimal_threads = self.cpu_count\n",
        "\n",
        "        # For very small data, fewer threads to avoid overhead\n",
        "        if data_size < 1000:\n",
        "            optimal_threads = max(1, self.cpu_count // 4)\n",
        "        elif data_size < 10000:\n",
        "            optimal_threads = max(2, self.cpu_count // 2)\n",
        "\n",
        "        # Cap at reasonable maximum to avoid thread switching overhead\n",
        "        return min(optimal_threads, 16)\n",
        "\n",
        "    def _get_initial_gpu_ratio(self, data_size, operation):\n",
        "        \"\"\"Get initial GPU ratio based on data size and operation type.\"\"\"\n",
        "        # If no GPU, use CPU only\n",
        "        if not self.has_gpu:\n",
        "            return 0.0\n",
        "\n",
        "        # Operations that typically benefit more from GPU\n",
        "        gpu_friendly_ops = {AggregateOperation.SUM, AggregateOperation.COUNT}\n",
        "\n",
        "        # Base ratio - higher for GPU-friendly operations\n",
        "        base_ratio = 0.7 if operation in gpu_friendly_ops else 0.5\n",
        "\n",
        "        # Adjust based on data size\n",
        "        if data_size < 5000:  # Small data might not be worth GPU overhead\n",
        "            return base_ratio * 0.5\n",
        "        elif data_size > 1000000:  # Large data benefits more from GPU\n",
        "            return min(0.9, base_ratio * 1.5)\n",
        "\n",
        "        return base_ratio\n",
        "\n",
        "    def _benchmark_ratios(self, data, operation, cpu_threads, ratios_to_try=None):\n",
        "        \"\"\"Benchmark different GPU ratios to find optimal performance.\"\"\"\n",
        "        if ratios_to_try is None:\n",
        "            ratios_to_try = [0.0, 0.3, 0.5, 0.7, 0.9]\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        for ratio in ratios_to_try:\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Run 3 times to get more stable measurements\n",
        "            for _ in range(3):\n",
        "                self.run_aggregate(data, operation, cpu_threads, ratio)\n",
        "\n",
        "            avg_time = (time.time() - start_time) / 3\n",
        "            results[ratio] = avg_time\n",
        "\n",
        "        # Find ratio with minimum execution time\n",
        "        optimal_ratio = min(results, key=results.get)\n",
        "        return optimal_ratio, results\n",
        "\n",
        "    def run_aggregate(self, values, operation, cpu_threads=None, gpu_ratio=None):\n",
        "        \"\"\"\n",
        "        Run aggregate operation with specified or auto-determined parameters.\n",
        "        This is a Python implementation that simulates the C library behavior.\n",
        "        \"\"\"\n",
        "        # Ensure values is a contiguous 1D array of int32\n",
        "        if not isinstance(values, np.ndarray):\n",
        "            values = np.array(values, dtype=np.int32)\n",
        "        elif values.dtype != np.int32:\n",
        "            values = values.astype(np.int32)\n",
        "\n",
        "        if values.ndim != 1:\n",
        "            raise ValueError(\"Input array must be 1-dimensional\")\n",
        "\n",
        "        # Auto-determine parameters if not specified\n",
        "        data_size = len(values)\n",
        "\n",
        "        if cpu_threads is None:\n",
        "            cpu_threads = self._get_optimal_cpu_threads(data_size)\n",
        "\n",
        "        if gpu_ratio is None:\n",
        "            # Check if we have a cached optimal ratio for similar workload\n",
        "            cache_key = (data_size // 1000, int(operation))\n",
        "            if cache_key in self._performance_cache:\n",
        "                gpu_ratio = self._performance_cache[cache_key]\n",
        "            else:\n",
        "                gpu_ratio = self._get_initial_gpu_ratio(data_size, operation)\n",
        "\n",
        "        # Validate parameters\n",
        "        if cpu_threads < 1:\n",
        "            raise ValueError(\"Number of CPU threads must be at least 1\")\n",
        "\n",
        "        if not 0.0 <= gpu_ratio <= 1.0:\n",
        "            raise ValueError(\"GPU ratio must be between 0.0 and 1.0\")\n",
        "\n",
        "        # Simulate hybrid processing by introducing small delays based on parameters\n",
        "        # This is just to demonstrate the concept - in reality we're using numpy\n",
        "\n",
        "        # Add a small delay based on gpu_ratio to simulate GPU time\n",
        "        if self.has_gpu and gpu_ratio > 0:\n",
        "            time.sleep(0.001 * gpu_ratio * min(1, data_size / 100000))\n",
        "\n",
        "        # Add a small delay based on cpu_threads to simulate thread overhead\n",
        "        if cpu_threads > 1:\n",
        "            time.sleep(0.0005 * min(cpu_threads, 8) * min(1, data_size / 100000))\n",
        "\n",
        "        # Use numpy operations which are already optimized\n",
        "        return python_aggregate(values, operation)\n",
        "\n",
        "    def find_optimal_parameters(self, data, operation, save_to_cache=True):\n",
        "        \"\"\"\n",
        "        Find optimal CPU/GPU parameters for this workload through benchmarking.\n",
        "        \"\"\"\n",
        "        data_size = len(data)\n",
        "\n",
        "        # Test different thread counts\n",
        "        thread_options = [1, 2, 4, 8, max(1, self.cpu_count)]\n",
        "        thread_options = sorted(list(set(thread_options)))  # Remove duplicates\n",
        "\n",
        "        best_threads = None\n",
        "        best_ratio = None\n",
        "        best_time = float('inf')\n",
        "\n",
        "        for threads in thread_options:\n",
        "            # Get initial GPU ratio estimate\n",
        "            initial_ratio = self._get_initial_gpu_ratio(data_size, operation)\n",
        "\n",
        "            # Fine-tune around the initial ratio\n",
        "            test_ratios = [max(0.0, initial_ratio - 0.2),\n",
        "                          initial_ratio,\n",
        "                          min(1.0, initial_ratio + 0.2)]\n",
        "\n",
        "            optimal_ratio, timing_results = self._benchmark_ratios(\n",
        "                data, operation, threads, test_ratios)\n",
        "\n",
        "            if timing_results[optimal_ratio] < best_time:\n",
        "                best_time = timing_results[optimal_ratio]\n",
        "                best_threads = threads\n",
        "                best_ratio = optimal_ratio\n",
        "\n",
        "        # Save to cache for future use\n",
        "        if save_to_cache:\n",
        "            cache_key = (data_size // 1000, int(operation))\n",
        "            self._performance_cache[cache_key] = best_ratio\n",
        "\n",
        "        return best_threads, best_ratio\n",
        "\n",
        "    def visualize_performance(self, data, operation):\n",
        "        \"\"\"\n",
        "        Create a visualization of performance across different CPU/GPU ratios.\n",
        "        \"\"\"\n",
        "        threads = self._get_optimal_cpu_threads(len(data))\n",
        "\n",
        "        # Test more granular ratios\n",
        "        ratios = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "        times = []\n",
        "\n",
        "        for ratio in ratios:\n",
        "            start = time.time()\n",
        "            # Run multiple times for more stable results\n",
        "            for _ in range(3):\n",
        "                self.run_aggregate(data, operation, threads, ratio)\n",
        "            avg_time = (time.time() - start) / 3\n",
        "            times.append(avg_time)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(ratios, times, 'o-', linewidth=2)\n",
        "        plt.xlabel('GPU Ratio')\n",
        "        plt.ylabel('Execution Time (s)')\n",
        "        plt.title(f'Performance vs. GPU Ratio for {operation.name} Operation\\n'\n",
        "                 f'(Data size: {len(data)}, CPU threads: {threads})')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "        best_ratio = ratios[times.index(min(times))]\n",
        "        print(f\"Optimal GPU ratio: {best_ratio:.2f}\")\n",
        "        return best_ratio\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Make sure we have the required packages\n",
        "        import subprocess\n",
        "        subprocess.check_call(['pip', 'install', 'psutil'])\n",
        "\n",
        "        # Create the adaptive aggregator\n",
        "        aggregator = AdaptiveHybridAggregator()\n",
        "\n",
        "        # Generate some test data\n",
        "        small_data = np.random.randint(0, 100, size=1000, dtype=np.int32)\n",
        "        medium_data = np.random.randint(0, 100, size=100000, dtype=np.int32)\n",
        "        large_data = np.random.randint(0, 100, size=10000000, dtype=np.int32)\n",
        "\n",
        "        print(\"Testing with different data sizes:\")\n",
        "\n",
        "        # Small data example\n",
        "        print(\"\\nSmall data (1,000 elements):\")\n",
        "        t_start = time.time()\n",
        "        optimal_threads, optimal_ratio = aggregator.find_optimal_parameters(small_data, AggregateOperation.SUM)\n",
        "        print(f\"Optimal parameters - CPU threads: {optimal_threads}, GPU ratio: {optimal_ratio:.2f}\")\n",
        "        result = aggregator.run_aggregate(small_data, AggregateOperation.SUM)\n",
        "        print(f\"Sum result: {result}\")\n",
        "        print(f\"Time with optimal parameters: {time.time() - t_start:.4f}s\")\n",
        "\n",
        "        # Compare with CPU-only\n",
        "        t_start = time.time()\n",
        "        result_cpu = aggregator.run_aggregate(small_data, AggregateOperation.SUM,\n",
        "                                             aggregator.cpu_count, 0.0)\n",
        "        cpu_time = time.time() - t_start\n",
        "        print(f\"CPU-only time: {cpu_time:.4f}s\")\n",
        "\n",
        "        # Medium data\n",
        "        print(\"\\nMedium data (100,000 elements):\")\n",
        "        t_start = time.time()\n",
        "        optimal_threads, optimal_ratio = aggregator.find_optimal_parameters(medium_data, AggregateOperation.SUM)\n",
        "        print(f\"Optimal parameters - CPU threads: {optimal_threads}, GPU ratio: {optimal_ratio:.2f}\")\n",
        "        result = aggregator.run_aggregate(medium_data, AggregateOperation.SUM)\n",
        "        print(f\"Sum result: {result}\")\n",
        "        print(f\"Time with optimal parameters: {time.time() - t_start:.4f}s\")\n",
        "\n",
        "        # Large data - visualize performance\n",
        "        print(\"\\nLarge data (10,000,000 elements):\")\n",
        "        print(\"Visualizing performance across different GPU ratios...\")\n",
        "        best_ratio = aggregator.visualize_performance(large_data, AggregateOperation.SUM)\n",
        "\n",
        "        # Performance comparison between operations\n",
        "        print(\"\\nComparing performance of different operations:\")\n",
        "        for op in AggregateOperation:\n",
        "            t_start = time.time()\n",
        "            result = aggregator.run_aggregate(medium_data, op)\n",
        "            op_time = time.time() - t_start\n",
        "            print(f\"{op.name}: {result} (time: {op_time:.4f}s)\")\n",
        "\n",
        "        print(\"\\nAdaptive CPU/GPU ratio demo completed!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"Please check if all required packages are installed.\")"
      ],
      "metadata": {
        "id": "49i2sDZtxvCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the aggregate function signature\n",
        "advanced_lib.execute_hybrid_aggregate.argtypes = [\n",
        "    np.ctypeslib.ndpointer(dtype=np.int32, ndim=1, flags='C_CONTIGUOUS'),  # values\n",
        "    ctypes.c_int,  # size\n",
        "    ctypes.POINTER(ctypes.c_int),  # result\n",
        "    ctypes.c_int,  # operation type (0: sum, 1: min, 2: max, 3: avg, 4: count)\n",
        "    ctypes.c_int,  # cpu threads\n",
        "    ctypes.c_float  # gpu ratio\n",
        "]"
      ],
      "metadata": {
        "id": "ZDzxJ-gZzhZL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}